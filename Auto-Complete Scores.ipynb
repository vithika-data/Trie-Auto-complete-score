{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A company has tasked you with gathering some qualitative metrics regarding a simple text search auto-complete feature. You'll be given a set of documents, each having a title and a body of text.\n",
    "\n",
    "Every word in the documents can be assigned a numeric score. The score is defined as follows:\n",
    "\n",
    "Each occurrence in the title: 10\n",
    "Each occurrence in the body: 1\n",
    "Note that the scores should be computed across all documents.\n",
    "\n",
    "For example, given two documents: \n",
    "\n",
    "#Title\t    #Body\n",
    "Document A\tANIMALS\tANT ANTELOPE CAMEL CAT DOG\n",
    "Document B\tDOG FACTS\tFURRY FURRY LOYAL\n",
    "\n",
    "ANIMALS has a score of 10, because it appears once in a document's title\n",
    "CAT has a score of 1, because it appears once in a document's body\n",
    "DOG has a score of 11, because it appears once in a docurnent's body, and once in a document's title\n",
    "FURRY has a score of 2, because it appears twice in a document's body\n",
    "You'll then be given a set of user queries, each a string with no whitespace. \n",
    "\n",
    "For each query, compute the highest score among all the words that could be auto-completed from it. For instance, among the set of words above, the query 'AN' could be auto-completed into ANIMALS, ANT, and ANTELOPE. If no such words exist, the score is 0.\n",
    "\n",
    "For example, given these following queries:\n",
    "\n",
    "AN would output 10, because it can auto-complete into ANIMAL (which has a higher score than ANT and ANTELOPE)\n",
    "DO would output 11, because it can auto-complete into DOG\n",
    "FUR would output 2, because it can auto-complete into FURRY\n",
    "ELEPH would output 0, because it cannot auto-complete into any of the words\n",
    "You can assume text and queries are comprised of A-Z characters. In documents, words are separated by a space; there is no other whitespace.\n",
    "\n",
    "Constraints\n",
    "N: the number of documents 1 <= N < 1,000\n",
    "Q: the number of queries 1 <= Q < 300,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentTitles=['AAA BBB', 'AA CDD CDD']\n",
    "documentBodies=['AAA CCC DDD', 'CCD','CCD']\n",
    "queries=['A', 'C', 'CC', 'X', 'BBB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AAA CCC DDD': 10, 'CCD': 20}\n",
      "{'AAA BBB': 1, 'AA CDD CDD': 1}\n"
     ]
    }
   ],
   "source": [
    "docBodyScore={i:documentBodies.count(i)*10 for i in documentBodies}\n",
    "docTitleScore={i:documentTitles.count(i) for i in documentTitles}\n",
    "print(docBodyScore)\n",
    "print(docTitleScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AA CDD CDD': 1, 'CCD': 20, 'AAA BBB': 1, 'AAA CCC DDD': 10}\n"
     ]
    }
   ],
   "source": [
    "docscore={k:docBodyScore.get(k,0)+docTitleScore.get(k,0) for k in set(docBodyScore)|set(docTitleScore)}\n",
    "print(docscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': [], 'C': [], 'CC': [], 'X': [], 'BBB': []}\n",
      "{'A': [], 'C': [], 'CC': [], 'X': [], 'BBB': []}\n"
     ]
    }
   ],
   "source": [
    "querysearch={}\n",
    "scorecalc={}\n",
    "for query in queries:\n",
    "    querysearch[query]=[]\n",
    "    scorecalc[query]=[]\n",
    "\n",
    "print(querysearch)\n",
    "print(scorecalc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ['AA CDD CDD', 'AAA BBB', 'AAA CCC DDD'], 'C': ['CCD'], 'CC': ['CCD'], 'X': [], 'BBB': []}\n"
     ]
    }
   ],
   "source": [
    "for query in querysearch:\n",
    "    for docs in docscore:\n",
    "        if docs.startswith(query):\n",
    "            querysearch[query].append(docs)\n",
    "            \n",
    "print(querysearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[1]\n",
      "1\n",
      "[1, 1]\n",
      "2\n",
      "[1, 1, 10]\n",
      "3\n",
      "10\n",
      "*************************\n",
      "C\n",
      "[20]\n",
      "1\n",
      "20\n",
      "*************************\n",
      "CC\n",
      "[20]\n",
      "1\n",
      "20\n",
      "*************************\n",
      "X\n",
      "20\n",
      "*************************\n",
      "BBB\n",
      "20\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "for query in querysearch:\n",
    "    print(query)\n",
    "    y=[]\n",
    "    for docs in querysearch[query]:\n",
    "        if len(docs)==0:\n",
    "            x.append(0)\n",
    "        y.append(int(docscore[docs]))\n",
    "        print(y)\n",
    "        print(len(y))\n",
    "    if len(y)==0:\n",
    "        x.append(0)\n",
    "    else:\n",
    "        b=max(y)\n",
    "        x.append(b)\n",
    "    print(b)\n",
    "    print('*************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    b=max(a)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "for val in docscore[docs]:\n",
    "            print(docs,val)\n",
    "            a.append(int(val))\n",
    "    scorecalc[query].append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': [], 'C': [], 'CC': [], 'X': [], 'BBB': []}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scorecalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
